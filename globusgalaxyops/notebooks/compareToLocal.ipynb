{
 "metadata": {
  "name": "",
  "signature": "sha256:e3188bac29e37d93cc9afff82a42fe474e129a1d8fbeafa19bf73cda8868a21a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, sys, re, zipfile, csv\n",
      "from pprint import pprint\n",
      "from bs4 import BeautifulSoup\n",
      "import pysam\n",
      "from Bio import SeqIO\n",
      "import pandas as pd\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MetricsCollector(object):\n",
      "    \n",
      "    def __init__(self, metrics_dir):\n",
      "        \n",
      "        self.dir = metrics_dir\n",
      "    \n",
      "    def get_metrics_list(self):\n",
      "        \n",
      "        self.ml = [ os.path.join(self.dir, f) \\\n",
      "                    for f in os.listdir(self.dir) \\\n",
      "                    if not re.search('^\\.', f)]\n",
      "\n",
      "    \n",
      "    def parse_picard_html(self, html_doc, source='table'):\n",
      "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
      "        table = soup.findAll('table', attrs={'cellpadding': '3'})\n",
      "\n",
      "        metric_dict = {}\n",
      "        \n",
      "        if len(table):\n",
      "            if source == 'table':\n",
      "                for tr in table[0].findAll('tr'):\n",
      "                    for td in tr.findAll('td'):\n",
      "                        if re.search('^([A-Z]+(\\_)*)+$', td.text):\n",
      "                            td_key = td.text.replace('\\n', '')\n",
      "                            td_val = td.next_sibling.string.replace(u'\\xa0', u'')\n",
      "                            td_val = td_val.replace('\\n', '')\n",
      "                            if not re.search('[a-z]', td_val.lower()) and len(td_val):\n",
      "                                td_val = float(td_val)\n",
      "                                metric_dict[td_key] = td_val\n",
      "            elif source == 'cell':             \n",
      "                for tr in table[0].findAll('tr'):\n",
      "                    for td in tr.findAll('td'):\n",
      "                        if re.search('^[A-Z]+', td.text):\n",
      "                            td_keys = td.text.split('\\t')\n",
      "                            td_vals = tr.next_sibling.next_sibling.text.split('\\t')\n",
      "                            td_keys = td_keys[0:len(td_vals)]\n",
      "                            metric_dict = { key: float(td_vals[idx]) for idx,key in enumerate(td_keys) }\n",
      "        else:\n",
      "            print html_doc\n",
      "\n",
      "\n",
      "        return metric_dict\n",
      "\n",
      "    def read_picard(self, metrics_file):\n",
      "        zfile = zipfile.ZipFile(metrics_file)\n",
      "        metric_html = [ f for f in zfile.namelist() if 'html' in f ][0]\n",
      "        if 'rna_seq' in metric_html.lower():\n",
      "            source = 'cell'\n",
      "        else:\n",
      "            source = 'table'\n",
      "\n",
      "        with zfile.open(metric_html) as f:\n",
      "            html_doc = f.read()\n",
      "            metric_dict = self.parse_picard_html(html_doc, source)\n",
      "        return metric_dict\n",
      "\n",
      "    def parse_text(self, metric_lines, source=None):\n",
      "        if source == 'htseq':\n",
      "            metric_dict = { l.split('\\t')[0].lstrip('__'): \\\n",
      "                            float(l.split('\\t')[1].strip()) \\\n",
      "                            for l in metric_lines }\n",
      "\n",
      "        elif source == 'tophat':\n",
      "            metric_dict = { re.sub(' ', '_', l.split('\\t')[1].strip()): \\\n",
      "                            l.split('\\t')[0] \\\n",
      "                            for l in metric_lines }\n",
      "            for m in metric_dict:\n",
      "                val = metric_dict[m]\n",
      "                if type(val) == str and re.search('%', val):\n",
      "                    new_val = float(re.search('[0-9]+(\\.[0-9]+)*', val).group()) / 100\n",
      "                    key = 'perc_' + m\n",
      "                    metric_dict[key] = new_val\n",
      "                    del(metric_dict[m])\n",
      "                else:\n",
      "                    metric_dict[m] = float(val)\n",
      "\n",
      "        return metric_dict\n",
      "\n",
      "    def read_text(self, metrics_file):\n",
      "        if 'mm' in metrics_file:\n",
      "            source = 'htseq'\n",
      "        else:\n",
      "            source = 'tophat'\n",
      "\n",
      "        with open(metrics_file, 'r') as f:\n",
      "            metric_lines = f.readlines()\n",
      "            metric_dict = self.parse_text(metric_lines, source)\n",
      "\n",
      "        return metric_dict\n",
      "    \n",
      "    def build_metric_df(self):\n",
      "        \n",
      "        if not hasattr(self, 'ml'):\n",
      "            self.get_metrics_list()\n",
      "        \n",
      "        lib_metric_dict = {}\n",
      "        for f in self.ml:\n",
      "\n",
      "            if not 'combined' in f:\n",
      "                metrics_file = os.path.join(self.dir, f)\n",
      "\n",
      "                lib = re.search('lib[0-9]+', metrics_file).group()\n",
      "\n",
      "                if 'zip' in metrics_file:\n",
      "                    metrics_dict = self.read_picard(metrics_file)\n",
      "                else:\n",
      "                    metrics_dict = self.read_text(metrics_file)\n",
      "\n",
      "                if lib in lib_metric_dict:\n",
      "                    lib_metric_dict[lib].update(metrics_dict)\n",
      "                else:\n",
      "                    lib_metric_dict[lib] = metrics_dict\n",
      "        \n",
      "        metric_df = pd.DataFrame(lib_metric_dict).transpose()\n",
      "        self.mdf = metric_df\n",
      "        \n",
      "    def write_metric_df(self, out_path):\n",
      "        \n",
      "        if not hasattr(self, 'mdf'):\n",
      "            self.build_metric_df()\n",
      "            \n",
      "        out_path += '_metrics.csv'\n",
      "        \n",
      "        self.mdf.to_csv(out_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CountsCollector(object):\n",
      "    def __init__(self, counts_dir):\n",
      "                \n",
      "        self.dir = counts_dir\n",
      "    \n",
      "    def get_counts_list(self):\n",
      "        \n",
      "        self.cl = [ os.path.join(self.dir, f) \\\n",
      "                    for f in os.listdir(self.dir) \\\n",
      "                    if not re.search('^\\.', f) ]\n",
      "\n",
      "    def get_lib_id(self, lib_file):\n",
      "        lib_id = re.search('lib[0-9]+(.*(XX)+)*', lib_file).group()\n",
      "\n",
      "        return lib_id\n",
      "\n",
      "    def build_count_dict(self):\n",
      "\n",
      "        if not hasattr(self, 'cl'):\n",
      "            self.get_counts_list()\n",
      "            \n",
      "        count_dict = {}\n",
      "        for idx,f in enumerate(self.cl):\n",
      "            \n",
      "            if not 'combined' in f:\n",
      "                lib = self.get_lib_id(f)\n",
      "                \n",
      "                with open(f) as cf:\n",
      "                    reader = csv.reader(cf, delimiter = '\\t')\n",
      "                    if idx == 0:\n",
      "                        count_header = ['geneName', lib]\n",
      "                        for row in reader:\n",
      "                            count_dict[row[0]] = [row[1]]\n",
      "                    else:\n",
      "                        count_header.append(lib)\n",
      "                        for row in reader:\n",
      "                            count_dict[row[0]].append(row[1])\n",
      "        \n",
      "        self.lcd = count_dict\n",
      "        self.lch = count_header\n",
      "        \n",
      "\n",
      "    def write_count_dict(self, out_path):\n",
      "\n",
      "        if not hasattr(self, 'lcd'):\n",
      "            self.build_count_dict()\n",
      "            \n",
      "        out_path += '_counts.csv'\n",
      "        \n",
      "        with open(out_path, 'w') as cf:\n",
      "            writer = csv.writer(cf)\n",
      "            writer.writerow(self.lch)\n",
      "            for entry in self.lcd:\n",
      "                writer.writerow([entry] + self.lcd[entry])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FileStatsCollector(object):\n",
      "    def __init__(self, proj_dir):\n",
      "                \n",
      "        self.dir = proj_dir\n",
      "    \n",
      "    def get_counts_list(self):\n",
      "        \n",
      "        self.cl = [ os.path.join(self.dir, f) \\\n",
      "                    for f in os.listdir(self.dir) \\\n",
      "                    if not re.search('^\\.', f) ]\n",
      "        \n",
      "    \n",
      "    def get_file_list(self, file_tag):\n",
      "        \n",
      "        file_tag += '$'\n",
      "        file_list = [ os.path.join(dp, f) \\\n",
      "                      for dp,dn,fn in os.walk(self.dir) \\\n",
      "                      for f in fn \\\n",
      "                      if re.search(file_tag, f)]\n",
      "        return file_list\n",
      "    \n",
      "    def get_lib_id(self, lib_file):\n",
      "        lib_id = re.search('lib[0-9]+(.*(XX)+)*', lib_file).group()\n",
      "\n",
      "        return lib_id\n",
      "    \n",
      "    def bufcount(self, filename):\n",
      "        f = open(filename)\n",
      "        lines = 0\n",
      "        buf_size = 1024 * 1024\n",
      "        read_f = f.read # loop optimization\n",
      "\n",
      "        buf = read_f(buf_size)\n",
      "        while buf:\n",
      "            lines += buf.count('\\n')\n",
      "            buf = read_f(buf_size)\n",
      "\n",
      "        return lines\n",
      "    \n",
      "    def get_size(self, filename):\n",
      "        file_size = os.stat(filename).st_size \n",
      "        \n",
      "        return file_size\n",
      "    \n",
      "    def count_bam_records(self, bam_file):\n",
      "        bam_records = list(pysam.AlignmentFile(bam_file, 'rb'))\n",
      "\n",
      "        return len(bam_records)\n",
      "    \n",
      "    def count_fasta_records(self, fasta_file):\n",
      "        handle = open(fasta_file, \"rU\")\n",
      "        fasta_records = list(SeqIO.parse(handle, \"fasta\"))\n",
      "        handle.close()\n",
      "\n",
      "        return len(fasta_records)\n",
      "\n",
      "    def count_fastq_records(self, fastq_file):\n",
      "        handle = open(fastq_file, \"rU\")\n",
      "        fastq_records = list(SeqIO.parse(handle, \"fastq\"))\n",
      "        handle.close()\n",
      "\n",
      "        return len(fastq_records)\n",
      "\n",
      "    def build_file_stat_dict(self):\n",
      "\n",
      "        file_type_list = ['fasta', 'fastq', 'bam']\n",
      "            \n",
      "        file_stat_dict = {}\n",
      "        for ft in file_type_list:\n",
      "            \n",
      "            file_list = self.get_file_list(ft)\n",
      "            file_list = [ f for f in file_list if 'noDups' not in f ]\n",
      "            for f in file_list:\n",
      "            \n",
      "                if not 'combined' in f:\n",
      "                    lib = self.get_lib_id(f)\n",
      "                    \n",
      "                    if not lib in file_stat_dict:\n",
      "                        file_stat_dict[lib] = {}\n",
      "\n",
      "                    file_stat_dict[lib][ft + '_lines'] = self.bufcount(f)\n",
      "                    file_stat_dict[lib][ft + '_size'] = self.get_size(f)\n",
      "                    \n",
      "                    if ft == 'bam':\n",
      "                        file_stat_dict[lib][ft + '_records'] = self.count_bam_records(f)\n",
      "\n",
      "                    elif ft == 'fasta':\n",
      "                        file_stat_dict[lib][ft + '_records'] = self.count_fasta_records(f)\n",
      "                        \n",
      "                    elif ft == 'fastq':\n",
      "                        file_stat_dict[lib][ft + '_records'] = self.count_fastq_records(f)\n",
      "\n",
      "        \n",
      "        self.fsd = file_stat_dict\n",
      "        \n",
      "\n",
      "    def write_file_stat_dict(self, out_path):\n",
      "\n",
      "        if not hasattr(self, 'fsd'):\n",
      "            self.build_file_stat_dict()\n",
      "            \n",
      "        out_path += '_file_stats.csv'\n",
      "        \n",
      "        pd.DataFrame(self.fsd).transpose().to_csv(out_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "globus_dir = \"/Volumes/genomics/Illumina/150615_D00565_0087_AC6VG0ANXX/gProject_P43-12Processed_151013/\"\n",
      "local_dir = \"/Volumes/genomics/Illumina/GLOBUS_TESTING_150615_D00565_0087_AC6VG0ANXX/Project_P43-12Processed_151021/\"\n",
      "\n",
      "# globus_dir = \"/Volumes/genomics/Illumina/150615_D00565_0087_AC6VG0ANXX/gProject_P109-1Processed_151001_formatted/\"\n",
      "# local_dir = \"/Volumes/genomics/Illumina/GLOBUS_TESTING/Project_P109-1Processed_151021/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def collect_project_info(project_dir, out_dir, source):\n",
      "    project = re.search('P[0-9]+\\-[0-9]+', project_dir).group()\n",
      "    \n",
      "    out_tag = project + '_' + source\n",
      "    out_path = os.path.join(out_dir, out_tag)\n",
      "    \n",
      "#     metrics_dir = os.path.join(os.path.abspath(project_dir), 'metrics')\n",
      "#     mc = MetricsCollector(metrics_dir)\n",
      "#     mc.write_metric_df(out_path)\n",
      "    \n",
      "#     counts_dir = os.path.join(os.path.abspath(project_dir), 'counts')\n",
      "#     cc = CountsCollector(counts_dir)\n",
      "#     cc.write_count_dict(out_path)\n",
      "\n",
      "    fsc = FileStatsCollector(project_dir)\n",
      "    fsc.write_file_stat_dict(out_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect_project_info(local_dir, \"../data\", \"local\")\n",
      "collect_project_info(globus_dir, \"../data\", \"globus\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "proj_dir = \"/Volumes/genomics/Illumina/150615_D00565_0087_AC6VG0ANXX/gProject_P43-12Processed_151013_formatted/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "project = re.search('P[0-9]+\\-[0-9]+', local_dir).group()\n",
      "out_dir = \"../data/\"\n",
      "source = \"local\"\n",
      "out_tag = project + '_' + source\n",
      "out_path = os.path.join(out_dir, out_tag)\n",
      "\n",
      "# fsc.write_file_stat_dict(out_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fsc = FileStatsCollector(local_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_list = fsc.get_file_list('fastq')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_fastq = file_list[0]\n",
      "test_fastq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'/Volumes/genomics/Illumina/GLOBUS_TESTING_150615_D00565_0087_AC6VG0ANXX/Project_P43-12Processed_151021/TrimmedFastqs/lib6834_C6VG0ANXX_trimmed.fastq'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    }
   ],
   "metadata": {}
  }
 ]
}
#!/usr/bin/env python
"""

"""
import logging
import os
import sys
import re

import argparse

from bripipetools import util
from bripipetools import genlims
from bripipetools import dbify
from bripipetools import monitoring
from bripipetools import postprocess


def parse_input_args(parser=None):
    parser.add_argument('-p', '--flowcell_path',
                        required=True,
                        default=None,
                        help=("path to flowcell run folder - e.g., "
                              "/mnt/genomics/Illumina/"
                              "150218_D00565_0081_BC5UF5ANXX/"))
    parser.add_argument('-t', '--output_type',
                        default='a',
                        choices=['c', 'm', 'q', 'v', 'a'],
                        help=("Select type of result file to combine: "
                              "c [counts], m [metrics], q [qc], "
                              "v [validation], a [all]"))
    parser.add_argument('-x', '--exclude_types',
                        default=[],
                        choices=['c', 'm', 'q', 'v'],
                        help=("Select type of result file to exclude: "
                              "c [counts], m [metrics], q [qc], "
                              "v [validation]"))
    parser.add_argument('-s', '--stitch_only',
                        action='store_true',
                        help=("Do NOT compile and merge all summary "
                              "(non-count) data into a single file at "
                              "the project level"))
    parser.add_argument('-c', '--clean_outputs',
                        action='store_true',
                        help="Attempt to clean/organize output files")
    parser.add_argument('-d', '--debug',
                        action='store_true',
                        help="Set logging level to debug")

    # Parse and collect input arguments
    return parser.parse_args()


def get_workflow_batches(flowcell_path):
    submissions = os.path.join(flowcell_path, 'globus_batch_submission')
    return (os.path.join(submissions, wb)
            for wb in os.listdir(submissions)
            if 'optimized' in wb)


def get_processed_projects(flowcell_path):
    return (os.path.join(flowcell_path, pp)
            for pp in os.listdir(flowcell_path)
            if re.search('Project_.*Processed', pp))


def main():
    parser = argparse.ArgumentParser()
    args = parse_input_args(parser)

    if args.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Import flowcell run details and raw data for sequenced libraries
    logger.info("importing raw data for flowcell at path {}"
                .format(args.flowcell_path))
    dbify.ImportManager(path=args.flowcell_path, db=genlims.db).run()

    workflow_batches = list(get_workflow_batches(args.flowcell_path))
    logger.info("found the following workflow batches: {}"
                .format(workflow_batches))

    # Check outputs of workflow batches
    genomics_root = util.matchdefault('.*(?=genomics)', args.flowcell_path)
    problem_count = 0
    for wb in workflow_batches:
        logger.info("checking outputs for workflow batch in file '{}'"
                    .format(wb))
        wb_outputs = monitoring.WorkflowBatchMonitor(
            workflowbatch_file=wb, genomics_root=genomics_root
            ).check_outputs()

        problem_outputs = filter(lambda x: x[1]['status'] != 'ok',
                                 wb_outputs.items())
        problem_count += len(problem_outputs)
        if len(problem_outputs):
            output_warnings = map(lambda x: 'OUTPUT {}: {}'.format(
                x[1]['status'].upper(), x[0]
                ), problem_outputs)
            for w in output_warnings:
                logger.warning(w)

    # Give option to continue
    if problem_count > 0:
        proceed = raw_input("{} problems encountered; proceed? (y/[n]): "
                            .format(problem_count))
        if proceed != 'y':
            logger.info("exiting program")
            sys.exit(1)
    else:
        logger.info("no problem outputs found")

    # Import workflow batch details and data for processed libraries
    for wb in workflow_batches:
        logger.info("importing processed data for workflow batch in file '{}'"
                    .format(wb))
        dbify.ImportManager(path=wb, db=genlims.db).run()

    processed_projects = list(get_processed_projects(args.flowcell_path))
    logger.info("found the following processed projects: {}"
                .format(processed_projects))

    # Post-process project files
    for pp in processed_projects:
        pp_short = os.path.basename(pp)
        if args.clean_outputs:
            logging.info("cleaning output files for '{}'"
                         .format(pp))
            postprocess.OutputCleaner(pp).clean_outputs()

        logger.info("combining output files for '{}' with option '{}'"
                    .format(pp_short, args.output_type))

        combined_paths = []
        if args.output_type in ['c', 'a'] and 'c' not in args.exclude_types:
            logger.info("generating combined counts file")
            path = os.path.join(pp, 'counts')
            postprocess.OutputStitcher(path).write_table()

        if args.output_type in ['m', 'a'] and 'm' not in args.exclude_types:
            logger.info("generating combined metrics file")
            path = os.path.join(pp, 'metrics')
            combined_paths.append(postprocess.OutputStitcher(path).write_table())

        if args.output_type in ['q', 'a'] and 'q' not in args.exclude_types:
            logger.info("generating combined QC file(s)")
            path = os.path.join(pp, 'QC')
            postprocess.OutputStitcher(path).write_overrepresented_seq_table()
            combined_paths.append(postprocess.OutputStitcher(path).write_table())

        if args.output_type in ['v', 'a'] and 'v' not in args.exclude_types:
            logger.info("generating combined validation file(s)")
            path = os.path.join(pp, 'validation')
            combined_paths.append(postprocess.OutputStitcher(path).write_table())

        if not args.stitch_only:
            logger.info("merging all combined summary data tables")
            postprocess.OutputCompiler(combined_paths).write_table()


if __name__ == "__main__":
    main()
